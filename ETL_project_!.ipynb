{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255bf0c6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "*Cliente*: Organización independiente pero adscrita al Ministerio de Trabajo.\n",
    "\n",
    "*Objetivo*: Analizar y visualizar datos relacionados con el paro nacional en España.\n",
    "\n",
    "*Variables*: \n",
    "1. Pandemia\n",
    "2. Género\n",
    "3. Nivel de formación\n",
    "\n",
    "*Catálogo de obtenicón de datos*: \"Mercado Laboral y Salarios\" \n",
    "\n",
    "*Orden de ETL*:\n",
    "1. Data Lake\n",
    "2. Data Warehouse\n",
    "3. Web Scraping\n",
    "\n",
    "*Visualizaciones*: 3 Dashboards en PowerBI \n",
    "1. Información General del Paro\n",
    "2. Efectos de la Pandemia en el empleo\n",
    "3. Diferencia de Paro según el nivel de formación alcanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd978d35",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215ba14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da3c8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DataLake = pd.read_csv(\"ETL-project-main\\ETL_project-postal_codes.csv\", sep=\";\")\n",
    "DataLake.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a6437",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En este punto del proceso, no hemos realizado transformaciones ni limpieza significativa de los datos. El objetivo principal del Data Lake es capturar y conservar todos los datos en su formato original, de manera que posteriormente podamos realizar el proceso de Extracción, Transformación y Carga (ETL) en el Data Warehouse.\n",
    "\n",
    "En la etapa del Data Lake, nos aseguramos de tener todos los datos disponibles para su posterior análisis, sin importar si provienen de diferentes fuentes o si están en formatos no estructurados. Una vez que los datos están almacenados en el Data Lake, podemos continuar con las siguientes etapas del proyecto, como la limpieza, transformación y carga en el Data Warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2462e",
   "metadata": {},
   "source": [
    "# Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7daa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ETL-project-main/ETL_project-postal_codes.csv\", encoding=\"latin1\")\n",
    "extension=len(file.readlines())\n",
    "file.close()\n",
    "lineas_de_texto=[]\n",
    "file = open(\"ETL-project-main/ETL_project-postal_codes.csv\", encoding=\"utf-8\")\n",
    "for i in range(extension):\n",
    "    fila=[]\n",
    "    variable = file.readline().split(\";\")\n",
    "    for item in variable:\n",
    "        if \"/\" in item:\n",
    "            fila.extend(item.split(\"/\"))\n",
    "        elif item.strip().isdigit() == False:\n",
    "            fila.extend([item,item])\n",
    "        else:\n",
    "            fila.append(item)\n",
    "    lineas_de_texto.append(fila)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39928750",
   "metadata": {},
   "source": [
    "En esta etapa del proyecto, nos encontramos trabajando en el Data Warehouse, donde llevamos a cabo el proceso de transformación de datos antes de cargarlos para su análisis y generación de informes.\n",
    "\n",
    "El código proporcionado realiza una serie de transformaciones en el conjunto de datos contenido en el archivo \"ETL_project-postal_codes.csv\". La intención es preparar los datos para que sean más adecuados para el análisis posterior. A continuación, se describen las principales transformaciones que se están realizando:\n",
    "\n",
    "1. Lectura del archivo: Primero, se lee el archivo \"ETL_project-postal_codes.csv\" utilizando la función `open()`, y se cuentan las líneas para determinar el tamaño del archivo.\n",
    "\n",
    "2. Procesamiento de las líneas de texto: Luego, se procede a leer el archivo nuevamente, esta vez utilizando el conjunto de caracteres \"utf-8\" para asegurar una correcta interpretación de los caracteres especiales. Se recorren las líneas del archivo en un bucle `for`, y para cada línea se realizan las siguientes operaciones:\n",
    "\n",
    "    a. Separación de elementos: Cada línea se divide utilizando el delimitador \";\" para obtener una lista de elementos.\n",
    "\n",
    "    b. Tratamiento de fechas: Si un elemento contiene una barra (\"/\"), se asume que representa una fecha en formato día/mes, por lo que se divide el elemento en dos partes (día y mes) y se agrega cada parte como elementos separados en la fila.\n",
    "\n",
    "    c. Duplicación de elementos no numéricos: Si el elemento no es un número, se duplica en la fila. Esto podría deberse a que el dato es importante y se necesita más de una vez en el análisis posterior.\n",
    "\n",
    "3. Almacenamiento de los datos transformados: Cada fila procesada se agrega a una lista llamada \"lineas_de_texto\", que contendrá todas las filas transformadas del archivo.\n",
    "\n",
    "Es importante tener en cuenta que en este punto, aún no se ha llevado a cabo la carga de los datos transformados en el Data Warehouse. El objetivo de esta sección del código es realizar las transformaciones necesarias para que los datos estén limpios y estructurados adecuadamente antes de cargarlos en la siguiente etapa del proceso.\n",
    "\n",
    "Una vez que se han realizado todas las transformaciones, el siguiente paso sería realizar la carga de los datos en el Data Warehouse, donde estarán listos para su análisis y generación de informes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a591c5",
   "metadata": {},
   "source": [
    "## Warehouse st.1 - Primera version CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo=open('archivo_salida.csv',\"w\",encoding=\"utf-8\")\n",
    "archivo.write(\"Provincia_local,Provincia,Ciudad_local,Ciudad,CP\\n\")\n",
    "for i in lineas_de_texto:\n",
    "    archivo.write((\",\").join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a145bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "provinces=pd.read_csv('archivo_salida.csv')\n",
    "provinces.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71b1fc",
   "metadata": {},
   "source": [
    "En esta etapa del Data Warehouse, estamos generando la primera versión de un archivo CSV que contiene datos transformados y limpios. El objetivo es preparar los datos para su posterior análisis y utilización.\n",
    "\n",
    "El código proporcionado realiza las siguientes acciones:\n",
    "\n",
    "1. Creación del archivo de salida: Se crea un nuevo archivo llamado 'archivo_salida.csv' utilizando la función `open()` con el modo de escritura (\"w\") y especificando la codificación \"utf-8\". Este archivo se utilizará para almacenar los datos transformados.\n",
    "\n",
    "2. Escritura de la cabecera: Se escribe la primera línea en el archivo, que contiene los nombres de las columnas o campos. En este caso, los nombres son \"Provincia_local\", \"Provincia\", \"Ciudad_local\", \"Ciudad\" y \"CP\", que representan las diferentes partes de los datos transformados.\n",
    "\n",
    "3. Escritura de los datos transformados: Luego, se recorre la lista \"lineas_de_texto\" que contiene los datos transformados generados en la etapa anterior. Para cada línea (fila) en la lista, se escriben los elementos (columnas) en el archivo 'archivo_salida.csv', separados por comas. Esto se logra mediante el método `join()` que une los elementos de la lista utilizando \",\" como separador.\n",
    "\n",
    "4. Lectura de los datos transformados: Finalmente, se utiliza la biblioteca `pandas` para leer el archivo 'archivo_salida.csv' en un DataFrame llamado \"provinces\". Esto permitirá tener los datos listos para su análisis y manipulación en el entorno de trabajo de Python.\n",
    "\n",
    "Es importante destacar que esta es la \"Warehouse st.1\", lo que significa que es la primera versión del archivo CSV con los datos transformados. En futuras etapas del Data Warehouse, es posible que se realicen más transformaciones o ajustes antes de que los datos estén completamente listos para su análisis y carga en sistemas de análisis como Power BI o herramientas similares.\n",
    "\n",
    "Al usar un archivo CSV como formato de almacenamiento, los datos se vuelven más accesibles y fáciles de compartir con otros equipos o herramientas para su posterior análisis y visualización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae1013",
   "metadata": {},
   "source": [
    "## QC del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1511d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(df):\n",
    "    import numpy as np\n",
    "    import pandas.api.types as api\n",
    "    reemplazos = {\" \": \"\", ',': '',\"'\": '',\"-\": \"\",\"\\\\x2E\": \"\",\"\\\\x27\": \"\",\"\\\\x28\":\"\",\"\\\\x29\": \"\", '\\\\x3F':\"\"}\n",
    "    Length=len(df)\n",
    "    file=open(\"Load_report.txt\", \"w\", encoding=\"utf-8\")\n",
    "    for i in range(len(df.columns)):\n",
    "        x=df[df.columns[i]]\n",
    "        name=df.columns[i]\n",
    "        file.write(f\"{name}\\n{Length} filas\\nValores no nulos:\\t{x.count()}\\nValores unicos:\\t{x.nunique()}\\nValores nulos:\\t{x.isnull().sum()}\\n\")\n",
    "        if api.is_numeric_dtype(df[df.columns[i]].dtype):\n",
    "            Real=df[df.columns[i]].apply(np.isreal)==False\n",
    "            file.write(f\"Valores no reales:\\t{Real.sum()}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "        elif api.is_string_dtype(x.dtype):\n",
    "            file.write(f\"Valores no alfanumericos:\\t{df[df.replace(reemplazos, regex=True)[df.columns[i]].str.isalnum() == False][df.columns[i]].value_counts()}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    file.close()\n",
    "    print(f\"Reporte de carga de datos generado en\\n Files/Load_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ee8f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report(provinces)   #txt generado para revision de valores anomalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces[provinces[\"Provincia_local\"]==\"﻿Araba\"]  # muestra de fila con valor anomalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces.loc[0,\"Provincia_local\"] = \"Araba\"  # reescritura de valor en campo 'Provincia_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces[provinces[\"Provincia_local\"]==\"﻿Araba\"] # rechequeo de existencia de filas con valores anomalos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b67d7",
   "metadata": {},
   "source": [
    "En esta etapa del proyecto, se está realizando un control de calidad (QC) del DataFrame `provinces` con el objetivo de identificar posibles valores anómalos o problemas en los datos. El código utiliza la biblioteca `pandas` y algunas funciones adicionales para generar un reporte de calidad y, si es necesario, corregir algunos valores anómalos encontrados.\n",
    "\n",
    "El código del control de calidad realiza las siguientes acciones:\n",
    "\n",
    "1. Definición de reemplazos: Se define un diccionario llamado `reemplazos` que contiene diferentes caracteres que se desean reemplazar en los datos. Esto puede ser útil para eliminar caracteres no deseados o espacios que podrían afectar el análisis posterior.\n",
    "\n",
    "2. Apertura del archivo de reporte: Se crea un nuevo archivo llamado \"Load_report.txt\" utilizando la función `open()` con el modo de escritura (\"w\") y especificando la codificación \"utf-8\". Este archivo se utilizará para almacenar el reporte de calidad generado.\n",
    "\n",
    "3. Generación del reporte: Para cada columna en el DataFrame, se realiza lo siguiente:\n",
    "   - Se extrae la columna `x` y el nombre de la columna `name`.\n",
    "   - Se escribe en el archivo \"Load_report.txt\" información relevante, como el nombre de la columna, la cantidad de filas en el DataFrame (`Length`), la cantidad de valores no nulos, la cantidad de valores únicos y la cantidad de valores nulos.\n",
    "   - Si la columna contiene datos numéricos, se cuenta la cantidad de valores que no son reales (no numéricos) y se agrega esta información al reporte.\n",
    "   - Si la columna contiene datos de tipo string (texto), se identifican los valores que no son alfanuméricos y se agrega esta información al reporte.\n",
    "\n",
    "4. Cierre del archivo de reporte: Se cierra el archivo \"Load_report.txt\" una vez que se ha generado el reporte completo.\n",
    "\n",
    "5. Corrección de valores anómalos: A continuación, se realiza una corrección específica para el valor anómalo \"﻿Araba\" encontrado en la columna \"Provincia_local\". Se busca la fila que contiene ese valor y se reemplaza con el valor correcto \"Araba\".\n",
    "\n",
    "6. Rechequeo de existencia de valores anómalos: Finalmente, se vuelve a buscar si hay filas con el valor anómalo \"﻿Araba\" en la columna \"Provincia_local\" para verificar que la corrección se haya realizado correctamente.\n",
    "\n",
    "Es importante realizar este control de calidad para asegurarnos de que los datos estén limpios, coherentes y listos para su análisis. La corrección de valores anómalos y la generación del reporte son parte fundamental del proceso de preparación de los datos para su uso en etapas posteriores del proyecto.\n",
    "\n",
    "El archivo \"Load_report.txt\" contiene el reporte de calidad generado y puede ser revisado para identificar cualquier problema o discrepancia en los datos antes de proceder con el análisis y la carga en sistemas de visualización como Power BI u otras herramientas similares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837a32d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Web scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d6ec0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scrapping de URL facilitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eaa579",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353befea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "webpage = 'https://servicios.ine.es/wstempus/js/es/DATOS_TABLA/33791?tip=AM' # URL path to scrap\n",
    "\n",
    "peticion = Request(webpage, headers={'User-Agent': 'Mozilla/5.0'}) #Establishing connection to the URL\n",
    "data_page = urlopen(peticion).read().decode('utf8')\n",
    "print(peticion)\n",
    "print(type(data_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d25409",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "started = data_page.split('\\n]\\n}\\n,')\n",
    "st1=[]\n",
    "for st in started:\n",
    "    if '\"Codigo\":\"MUN\"' in st:\n",
    "        st1.append(st)\n",
    "print(f\"{len(st1)} registros encontrados\")\n",
    "Data_standard=[]\n",
    "for item in st1:\n",
    "    fila={}\n",
    "    raw = item.split(', \"Data\":')\n",
    "    metajson = json.loads(raw[0]+\"}\")  #Se agrega la llave que no esta incluida en los items intermedios de la lista resutante del split\n",
    "    metadata=metajson[\"Nombre\"].split(\".\")\n",
    "    data={}\n",
    "    rawData = (raw[1].lstrip(\"[\").rstrip(\"\\n]\\n}\\n]\\n\")+\"}\").split(\"\\n,\")\n",
    "    nAños= len(rawData)\n",
    "    for i in range (nAños):\n",
    "        record = json.loads(rawData[i])\n",
    "        data.update({record[\"Anyo\"]:record[\"Valor\"]})\n",
    "    fila.update({\"ID\" : metajson[\"COD\"]})\n",
    "    fila.update({\"Muestra de poblacion: \": metadata[0]})\n",
    "    fila.update({\"Poblado: \": metadata[1]})\n",
    "    fila.update({\"Procedencia de poblacion: \": (\" \").join(metadata[2:4])})\n",
    "    fila.update({\"Provincia\" : (\", \").join(metajson[\"MetaData\"][1]['T3_JerarquiaPadres'])})\n",
    "    fila.update(data)\n",
    "    Data_standard.append(fila)\n",
    "# print(Data_standard,\"\\n\",sep=\"\\n\")\n",
    "pd.DataFrame(Data_standard,index=list(range(len(Data_standard))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b03906",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Web scraping - Scrapping de URL facilitado\n",
    "\n",
    "En esta etapa del proyecto, estamos realizando web scraping de una URL específica para obtener datos. El objetivo es extraer información de la página web y estructurarla en un formato adecuado para su posterior análisis y utilización.\n",
    "\n",
    "1. Establecimiento de la conexión y obtención de datos:\n",
    "   - Se importan las bibliotecas necesarias, incluyendo `urllib.request` para establecer la conexión y `json` para trabajar con los datos en formato JSON.\n",
    "   - Se proporciona la URL en la variable `webpage` que contiene la página web que deseamos rascar.\n",
    "   - Se establece una conexión a la URL utilizando la función `Request()` y se especifica un User-Agent para simular una solicitud de navegador, evitando posibles bloqueos por parte del servidor. Luego, se lee el contenido de la página web en formato `utf8`.\n",
    "\n",
    "2. Separación y filtrado de datos:\n",
    "   - El contenido de la página web se divide en una lista de elementos utilizando el carácter '\\n]\\n}\\n,' como separador. Se busca la presencia de registros que contengan la clave `\"Codigo\":\"MUN\"`, lo que indica que son los datos que nos interesan. Estos registros se recopilan en la lista `st1`.\n",
    "\n",
    "3. Estructuración de los datos:\n",
    "   - Para cada elemento en `st1`, se realiza lo siguiente:\n",
    "     - Se separa el texto en dos partes usando la cadena ', \"Data\":' como delimitador. La primera parte contiene metadatos sobre la muestra de población, poblado, procedencia de población y provincia, mientras que la segunda parte contiene información de años y sus respectivos valores.\n",
    "     - Se utiliza `json.loads()` para convertir la primera parte en un diccionario. Se extraen metadatos relevantes y se almacenan en variables.\n",
    "     - Se divide la segunda parte en una lista de registros JSON correspondientes a cada año y valor.\n",
    "     - Se itera sobre los registros JSON y se extraen los datos de año y valor, que se almacenan en un diccionario `data`.\n",
    "     - Se crea un diccionario `fila` para almacenar los metadatos y los datos de año y valor.\n",
    "     - Se agrega el diccionario `fila` a la lista `Data_standard` que contendrá todos los datos estructurados.\n",
    "\n",
    "4. Creación de DataFrame:\n",
    "   - Se crea un DataFrame de pandas a partir de la lista `Data_standard`, que muestra los datos en forma de tabla, listando cada fila con sus respectivos metadatos y valores para cada año.\n",
    "\n",
    "En esta etapa del proyecto, hemos extraído datos de la página web y los hemos estructurado en un formato tabular (DataFrame). A partir de aquí, podemos realizar análisis, limpieza adicional o cargar los datos en el Data Warehouse para su posterior uso en la presentación y visualización de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0f34c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Conexion mediante Api a catalogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440aede",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword=\"DPOP\"\n",
    "url = f\"https://datos.gob.es/apidata/catalog/dataset/keyword/{keyword}\"\n",
    "x = requests.get(url, params= {\"_sort\": \"items,description\", \"_pageSize\": 200})\n",
    "print(\"Request response code: \",x.status_code, sep=\"\\n\")\n",
    "respuesta = x.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f91c7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numero_respuestas = len(respuesta['result'][\"items\"])\n",
    "accessPoint =[]\n",
    "for n in range(numero_respuestas):\n",
    "    description = respuesta['result'][\"items\"][n][\"description\"][0][\"_value\"][16:]\n",
    "    notaccessURL = respuesta['result'][\"items\"][n][\"distribution\"]\n",
    "    for i in notaccessURL:\n",
    "        if \"json\" in (i[\"format\"][\"value\"]):\n",
    "            accessURL = notaccessURL[notaccessURL.index(i)]['accessURL']\n",
    "    accessPoint.append({\"description\":description, \"URL\" :accessURL})\n",
    "Endpoints = pd.DataFrame(accessPoint, index=list(range(len(accessPoint))))\n",
    "URLmart = Endpoints[Endpoints[\"description\"].str.contains(\": Población por municipios y sexo.\")==True]\n",
    "Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901a343",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En esta etapa del proyecto, estamos conectando a través de una API a un catálogo de datos para obtener información relacionada con el término de búsqueda \"DPOP\". El objetivo es obtener información relevante y estructurarla en un DataFrame para su posterior análisis y utilización.\n",
    "\n",
    "1. Realización de la solicitud y obtención de datos:\n",
    "   - Se importa la biblioteca `requests` para realizar la conexión y obtener los datos a través de una solicitud HTTP.\n",
    "   - Se define la variable `keyword` con el término de búsqueda \"DPOP\".\n",
    "   - Se forma la URL de la API a través de la concatenación de la variable `keyword` en la URL base.\n",
    "   - Se realiza una solicitud GET a la URL de la API y se aplican parámetros adicionales como `_sort` y `_pageSize` para ordenar y limitar la cantidad de resultados devueltos.\n",
    "   - Se imprime el código de respuesta de la solicitud HTTP para verificar que la conexión fue exitosa.\n",
    "\n",
    "2. Procesamiento de la respuesta de la API:\n",
    "   - Se convierte la respuesta de la API en formato JSON utilizando `x.json()` y se almacena en la variable `respuesta`.\n",
    "   - Se obtiene el número de respuestas (registros) en el catálogo utilizando `len(respuesta['result'][\"items\"])`.\n",
    "   - Se crea una lista llamada `accessPoint` para almacenar la información relevante que se extraerá del catálogo.\n",
    "\n",
    "3. Extracción de información relevante:\n",
    "   - Se recorre cada registro en la respuesta de la API utilizando un bucle `for`.\n",
    "   - Se obtiene la descripción del registro y se almacena en la variable `description`.\n",
    "   - Se busca la URL de acceso a los datos en formato JSON dentro del registro y se almacena en la variable `accessURL`.\n",
    "   - La información de `description` y `accessURL` se agrega al diccionario `accessPoint` en cada iteración del bucle.\n",
    "\n",
    "4. Creación del DataFrame:\n",
    "   - Se crea un DataFrame de pandas llamado `Endpoints` a partir de la lista `accessPoint`, que mostrará la descripción y la URL de acceso a los datos para cada registro.\n",
    "   - Se filtra el DataFrame para obtener solo los registros que contienen la descripción \": Población por municipios y sexo.\" y se almacena en el DataFrame `URLmart`.\n",
    "\n",
    "En esta etapa del proyecto, hemos obtenido información específica del catálogo a través de la API y la hemos estructurado en un formato tabular (DataFrame). A partir de aquí, podemos utilizar esta información para realizar análisis, presentar datos o cualquier otra tarea requerida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b068cbb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Consulta en serie automatizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f651b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def requesting(webpage):\n",
    "    from urllib.request import Request, urlopen\n",
    "    try:\n",
    "        peticion = Request(webpage, headers={'User-Agent': 'Mozilla/5.0'}) #Establishing connection to the URL\n",
    "        data_page = urlopen(peticion).read().decode('utf8')\n",
    "        return data_page\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d005877",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataframing(data_page, name = \"File_result\"):\n",
    "    import json\n",
    "    started = data_page.split('\\n]\\n}\\n,')\n",
    "    st1=[]\n",
    "    for st in started:\n",
    "        if '\"Codigo\":\"MUN\"' in st:\n",
    "            st1.append(st)\n",
    "    print(f\"{len(st1)} registros encontrados\")\n",
    "    Data_standard=[]\n",
    "    for item in st1:\n",
    "        fila={}\n",
    "        raw = item.split(', \"Data\":')\n",
    "        metajson = json.loads(raw[0]+\"}\")  #Se agrega la llave que no esta incluida en los items intermedios de la lista resutante del split\n",
    "        metadata=metajson[\"Nombre\"].split(\".\")\n",
    "        data={}\n",
    "        rawData = (raw[1].lstrip(\"[\").rstrip(\"\\n]\\n}\\n]\\n\")+\"}\").split(\"\\n,\")\n",
    "        nAños= len(rawData)\n",
    "        for i in range (nAños):\n",
    "            record = json.loads(rawData[i])\n",
    "            data.update({record[\"Anyo\"]:record[\"Valor\"]})\n",
    "        fila.update({\"ID\" : metajson[\"COD\"]})\n",
    "        fila.update({\"Muestra de poblacion: \": metadata[0]})\n",
    "        fila.update({\"Poblado: \": metadata[1]})\n",
    "        fila.update({\"Procedencia de poblacion: \": (\" \").join(metadata[2:4])})\n",
    "        for c in range(len(metajson[\"MetaData\"])):\n",
    "            if 'T3_JerarquiaPadres' in metajson[\"MetaData\"][c].keys():\n",
    "                fila.update({\"Provincia\" : (\", \").join(metajson[\"MetaData\"][c]['T3_JerarquiaPadres'])})\n",
    "        fila.update(data)\n",
    "        Data_standard.append(fila)\n",
    "    df = pd.DataFrame(Data_standard,index=list(range(len(Data_standard))))\n",
    "    df.to_csv('name.csv',sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214eabc8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "links = URLmart[\"URL\"].values\n",
    "description = URLmart[\"description\"].values\n",
    "webpage = links[49]\n",
    "data_page = requesting(webpage)\n",
    "final = dataframing(data_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eb08b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En esta etapa del proyecto, se ha creado una función para automatizar el proceso de consulta a través de una lista de enlaces web (URLs) y la creación de DataFrames a partir de los datos obtenidos. El proceso se realiza en dos funciones distintas: `requesting()` y `dataframing()`.\n",
    "\n",
    "1. Función `requesting(webpage)`:\n",
    "   - Esta función se encarga de realizar una solicitud HTTP a una URL específica.\n",
    "   - Utiliza la biblioteca `urllib.request` para establecer la conexión a la URL y obtener los datos de la página web.\n",
    "   - Si la conexión es exitosa, devuelve los datos de la página web en formato de texto.\n",
    "\n",
    "2. Función `dataframing(data_page, name=\"File_result\")`:\n",
    "   - Esta función se encarga de procesar los datos de la página web y estructurarlos en un DataFrame.\n",
    "   - Los datos de la página web se dividen en una lista llamada `st1`, que contiene registros específicos que contienen la clave `\"Codigo\":\"MUN\"`.\n",
    "   - Se recorre cada registro en `st1`, y se extrae la información relevante como metadatos y valores para cada año. Luego, se estructura en un diccionario llamado `fila`.\n",
    "   - Se agrega cada diccionario `fila` a la lista `Data_standard`.\n",
    "   - Finalmente, se crea un DataFrame de pandas llamado `df` a partir de la lista `Data_standard`, y se guarda en un archivo CSV con el nombre especificado en la variable `name`.\n",
    "\n",
    "3. Automatización de la consulta y creación de DataFrames:\n",
    "   - Se obtiene la lista de URLs de la variable `links`.\n",
    "   - Se recorre la lista de URLs en un bucle `for`.\n",
    "   - En cada iteración, se realiza una consulta a la URL utilizando la función `requesting(webpage)` y se almacenan los datos de la página web en la variable `data_page`.\n",
    "   - Luego, se utiliza la función `dataframing(data_page)` para procesar los datos y crear el DataFrame correspondiente.\n",
    "\n",
    "En esta etapa del proyecto, se están automatizando las consultas y creaciones de DataFrames para cada URL en la lista `links`. Cada DataFrame se guarda en un archivo CSV con un nombre específico. Este enfoque es útil cuando se tiene una lista de enlaces web de donde se desea extraer datos de manera automatizada y estructurada para su posterior análisis y manipulación.\n",
    "\n",
    "Es importante tener en cuenta que el código se encuentra en proceso y puede requerir ajustes o mejoras adicionales según las necesidades específicas del proyecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514adb87",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Obtencion de base de datos para presentacion en PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993e37c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword=\"Mercado laboral y salarios\"\n",
    "url = f\"https://datos.gob.es/apidata/catalog/dataset/keyword/{keyword}\"\n",
    "x = requests.get(url, params= {\"_sort\": \"items,description\", \"_pageSize\": 200, \"_page\":8})\n",
    "print(\"Request response code: \",x.status_code, sep=\"\\n\")\n",
    "respuesta = x.json()\n",
    "numero_respuestas = len(respuesta['result'][\"items\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2d1d0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accessPoint =[]\n",
    "for n in range(numero_respuestas):\n",
    "    description = respuesta['result'][\"items\"][n][\"description\"][0][\"_value\"]\n",
    "    notaccessURL = respuesta['result'][\"items\"][n][\"distribution\"]\n",
    "    for i in notaccessURL:\n",
    "        if \"json\" in (i[\"format\"][\"value\"]):\n",
    "            accessURL = notaccessURL[notaccessURL.index(i)]['accessURL']\n",
    "    accessPoint.append({\"description\":description, \"URL\" :accessURL})\n",
    "Endpoints = pd.DataFrame(accessPoint, index=list(range(len(accessPoint))))\n",
    "URLmart = Endpoints[Endpoints[\"description\"].str.contains(\"Tasas de paro por nivel de formación alcanzado, sexo y grupo de edad. Anual\")==True]\n",
    "URLmart[\"URL\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581aa2f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for a in range(len(data)):\n",
    "            print({\"Periodo\":data[a][0]})\n",
    "            print({\"Total (%)\":data[a][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fa5fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dfing2 (data_page):\n",
    "    import json\n",
    "    started = data_page.split('\\n}\\n,')\n",
    "    Data_standard=[]\n",
    "    for item in started:\n",
    "        raw = item.split(', \"Data\":')\n",
    "        metajson = json.loads(raw[0].strip().lstrip(\"[\")+\"}\")\n",
    "        metadata=metajson[\"Nombre\"].split(\".\")\n",
    "        data=[]\n",
    "        rawData = (raw[1].lstrip(\"[\").rstrip(\"\\n]\\n}\\n]\\n\")+\"}\").split(\"\\n,\")\n",
    "        nAños= len(rawData)\n",
    "        for i in range (nAños):\n",
    "            record = json.loads(rawData[i])\n",
    "            data.append([record[\"Anyo\"],record[\"Valor\"]])\n",
    "        for a in range(len(data)):\n",
    "            fila={}\n",
    "            fila.update({\"Sexo\" : metadata[2]})\n",
    "            fila.update({\"Edad\": metadata[3].strip()})\n",
    "            fila.update({\"Nivel de formacion\": metadata[4]})\n",
    "            fila.update({\"Periodo\":data[a][0]})\n",
    "            fila.update({\"Total (%)\":data[a][1]})\n",
    "            Data_standard.append(fila)\n",
    "    return Data_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b838ecd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "links = URLmart[\"URL\"].values\n",
    "for link in links:\n",
    "    webpage = link\n",
    "    data_page = requesting(webpage)\n",
    "    dataset=dfing2(data_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2985524",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sexo            Edad   Nivel de formación alcanzado  Periodo   Total\n",
    "Ambos sexos     Total              Total              2022      12,92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639f22e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50bc84",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En esta etapa del proyecto, se ha realizado la obtención de una base de datos específica relacionada con el \"Mercado laboral y salarios\" desde una API. El objetivo es estructurar esta información en un DataFrame para su posterior presentación en Power BI o cualquier otra herramienta de visualización.\n",
    "\n",
    "1. Realización de la solicitud y obtención de datos:\n",
    "   - Se define la variable `keyword` con el término de búsqueda \"Mercado laboral y salarios\".\n",
    "   - Se forma la URL de la API a través de la concatenación de la variable `keyword` en la URL base.\n",
    "   - Se realiza una solicitud GET a la URL de la API y se aplican parámetros adicionales como `_sort`, `_pageSize` y `_page` para ordenar, limitar la cantidad de resultados y obtener una página específica de resultados.\n",
    "   - Se imprime el código de respuesta de la solicitud HTTP para verificar que la conexión fue exitosa.\n",
    "\n",
    "2. Procesamiento de la respuesta de la API:\n",
    "   - La respuesta de la API se convierte en formato JSON utilizando `x.json()` y se almacena en la variable `respuesta`.\n",
    "   - Se obtiene el número de respuestas (registros) en el catálogo utilizando `len(respuesta['result'][\"items\"])`.\n",
    "   - Se crea una lista llamada `accessPoint` para almacenar la información relevante que se extraerá del catálogo.\n",
    "\n",
    "3. Extracción de información relevante:\n",
    "   - Se recorre cada registro en la respuesta de la API utilizando un bucle `for`.\n",
    "   - Se obtiene la descripción del registro y se almacena en la variable `description`.\n",
    "   - Se busca la URL de acceso a los datos en formato JSON dentro del registro y se almacena en la variable `accessURL`.\n",
    "   - La información de `description` y `accessURL` se agrega al diccionario `accessPoint` en cada iteración del bucle.\n",
    "\n",
    "4. Estructuración de la base de datos:\n",
    "   - Se obtienen los enlaces de acceso a los datos en formato JSON desde el DataFrame `URLmart[\"URL\"].values`.\n",
    "   - Para cada enlace, se realiza una solicitud a la URL utilizando la función `requesting(webpage)` y se almacenan los datos de la página web en la variable `data_page`.\n",
    "   - Luego, se utiliza la función `dfing2(data_page)` para procesar los datos y crear una lista de diccionarios llamada `dataset`, que contiene la información estructurada.\n",
    "\n",
    "5. Creación del DataFrame final:\n",
    "   - Se crea un DataFrame de pandas llamado `df` a partir de la lista de diccionarios `dataset`.\n",
    "   - El DataFrame resultante contendrá la información sobre el sexo, edad, nivel de formación alcanzado, período y el total (%) correspondiente.\n",
    "\n",
    "En esta etapa del proyecto, hemos obtenido una base de datos específica desde la API, la hemos estructurado en un formato tabular (DataFrame) y estamos listos para presentarla y analizarla en Power BI o cualquier otra herramienta de visualización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048d03e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DataMart "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd22a3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## CSV de provincias standarizado y limpiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8d8ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "provinces.to_csv('Provincias.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dbbfbb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En esta etapa del proyecto, se ha creado un DataFrame llamado `provinces` que contiene datos sobre provincias obtenidos anteriormente a través del proceso de web scraping y limpieza.\n",
    "\n",
    "1. Creación del archivo CSV:\n",
    "   - Se utiliza el método `to_csv()` del DataFrame `provinces` para exportar los datos a un archivo CSV.\n",
    "   - El nombre del archivo se establece como \"Provincias.csv\".\n",
    "\n",
    "2. Guardado del archivo CSV:\n",
    "   - El archivo CSV se guarda en el directorio de trabajo actual con el nombre \"Provincias.csv\".\n",
    "   - Los datos en el archivo CSV estarán organizados y limpios, listos para ser utilizados en otras etapas del proyecto o para su análisis y presentación en herramientas como Power BI.\n",
    "\n",
    "En esta etapa del proyecto, se ha completado la creación y guardado del archivo CSV que contiene información sobre provincias estandarizada y limpiada. Este archivo CSV puede ser útil para cargar los datos en el Data Warehouse o para su uso directo en herramientas de visualización o análisis de datos como Power BI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "624.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
